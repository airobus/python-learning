import json
from fastapi.responses import FileResponse
from fastapi import FastAPI
# from config import SUPABASE
from pathlib import Path
import json
from langchain.schema import messages_to_dict
import logging
from langchain_community.chat_message_histories import FileChatMessageHistory
from robus.config import SUPABASE, ms_llm, cf_llm, qw_llm, qw_llm_openai, groq_llm_openai, conversationChain, \
    chroma_retriever, embeddings, redis_chat_history, get_message_history
from fastapi.responses import Response, StreamingResponse, JSONResponse
from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory, \
    ConversationBufferWindowMemory, ConversationSummaryBufferMemory
from langchain.chains.conversation.base import ConversationChain
from typing import AsyncIterable, Awaitable, Iterator
import asyncio
from typing import Dict, Any, Iterator
from starlette.schemas import OpenAPIResponse
from langchain_core.messages import BaseMessageChunk, BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain.schema.runnable import RunnablePassthrough, RunnableConfig
from langchain.load.serializable import Serializable
from typing import Optional, Dict
from langchain_core.runnables.utils import Input
from langchain.schema.runnable import Runnable
# from langchain import PromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from operator import itemgetter
from langchain_community.document_loaders import UnstructuredURLLoader
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import SupabaseVectorStore
from langchain.chains.summarize import load_summarize_chain
from langchain.retrievers.multi_query import MultiQueryRetriever
import logging
from langchain_core.runnables.history import RunnableWithMessageHistory
import datetime

# >>>>>>>>>>åŸºç¡€>>>>>>>>>>>>>>
log = logging.getLogger(__name__)
log.setLevel("INFO")

logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

app = FastAPI()


# >>>>>>>>>>æ¥å£>>>>>>>>>>>>>>>
# è¿”å›é¡µé¢
@app.get("/")
async def chroma_homepage():
    file_path = Path(__file__).parent / "index.html"
    return FileResponse(file_path)


# è¿”å›é¡µé¢2
@app.get("/upload")
async def chroma_upload():
    file_path = Path(__file__).parent / "upload.html"
    return FileResponse(file_path)


@app.post("/ask")
def ask(body: dict):
    return Response(call_llm(body['question']))


@app.post("/v2/stream/rag/memory/user/ask")
def ask(body: dict):
    question = body['question']
    user_id = 888

    system_prompt = get_prompt_en()

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    today = datetime.date.today()
    current_date = today.strftime("%Y%m%d")
    path = f'/Users/pangmengting/Documents/workspace/python-learning/data/history/conversation_{current_date}'

    memory = get_memory(path, user_id)
    rag_chain_from_docs = (
            RunnablePassthrough.assign(
                context=(lambda x: format_docs(x["retriever_context"])),
                history=memory.load_memory_variables
            )
            | prompt
            | qw_llm_openai
            | StrOutputParser()
    )
    retrieve_docs = (lambda x: x["input"]) | chroma_retriever

    chain = (
        RunnablePassthrough.assign(retriever_context=retrieve_docs)
        .assign(answer=rag_chain_from_docs)
        .assign(
            memory_update=lambda x: memory.save_context(
                {"input": x["input"]},
                {"output": x["answer"]}
            )
        )
    )

    # return chain.invoke({"input": question})["answer"]
    # æµå¼è¿”å›ï¼Œç»è¿”å›["answer"]çš„å€¼
    def generate():
        # Iterator[Output]:
        for chunk in chain.stream({"input": question}):
            if "answer" in chunk:
                # ç”ŸæˆåŒ…å« answer å’Œ retriever_context ä¸­æ¯ä¸ªæ–‡æ¡£çš„ page_content å’Œ metadata çš„å­—ç¬¦ä¸²
                yield f"{chunk['answer']}"
            # if "retriever_context" in chunk:
            #     yield f"sourceï¼š\n\n"
            #     for index, doc in enumerate(chunk["retriever_context"]):
            #         yield f"\n{doc.metadata['source']}\n"

    # if "answer" in chunk:
    #     yield f"{chunk['answer']}"

    return StreamingResponse(generate(), media_type="text/event-stream")


@app.post("/v2/stream/rag/memory/ask")
def ask(body: dict):
    question = body['question']

    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        "{context}"
        "\n\n"
        "Previous conversation:\n{history}"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )
    # print(path) /Users/pangmengting/Documents/workspace/python-learning/robus/api
    # memory1
    # memory = ConversationBufferMemory(return_messages=True)

    # memory2
    path = '/Users/pangmengting/Documents/workspace/python-learning/data/history/conversation_20240709-robus.json'
    message_history = CustomFileChatMessageHistory(file_path=path)
    # memory = ConversationBufferMemory(chat_memory=message_history, return_messages=True)

    # memory3
    memory = ConversationBufferWindowMemory(k=2, chat_memory=message_history, return_messages=True)

    rag_chain_from_docs = (
            RunnablePassthrough.assign(
                context=(lambda x: format_docs(x["retriever_context"])),
                history=memory.load_memory_variables
            )
            | prompt
            | qw_llm_openai
            | StrOutputParser()
    )
    retrieve_docs = (lambda x: x["input"]) | chroma_retriever

    chain = (
        RunnablePassthrough.assign(retriever_context=retrieve_docs)
        .assign(answer=rag_chain_from_docs)
        .assign(
            memory_update=lambda x: memory.save_context(
                {"input": x["input"]},
                {"output": x["answer"]}
            )
        )
    )

    # return chain.invoke({"input": question})["answer"]
    # æµå¼è¿”å›ï¼Œç»è¿”å›["answer"]çš„å€¼
    def generate():
        # Iterator[Output]:
        for chunk in chain.stream({"input": question}):
            if "answer" in chunk:
                yield f"{chunk['answer']}"

    return StreamingResponse(generate(), media_type="text/event-stream")


# stream + rag + memory âŒ æš‚æ—¶æ²¡æœ‰memory
@app.post("/stream/rag/memory/ask")
def ask(body: dict):
    question = body['question']

    chat_prompt = ChatPromptTemplate.from_template(prompt())
    chat_history = conversationChain.memory.buffer
    print(chat_history)

    # é¢„å¤„ç†è¾“å…¥çš„æ•°æ®
    def pre_input(prompt: str) -> Dict:
        # rag
        context = str(chroma_retriever | format_docs)
        return {
            "context": context,
            "question": prompt,
            "chat_history": chat_history
        }

    chain = (
            RunnableLambda(pre_input) |
            {
                "context": itemgetter('context'),
                "question": itemgetter('question'),
                "chat_history": itemgetter('chat_history'),
            }
            | chat_prompt
            | RunnablePassthrough()
            | StdOutputRunnable()
            | qw_llm_openai
            | StrOutputParser()
    )

    # æµå¼è¿”å›
    def generate():
        # Iterator[Output]:
        for chunk in chain.stream(question):
            for key in chunk:
                yield key

    return StreamingResponse(generate(), media_type="text/event-stream")


# åŸºäº: rag + memory çš„é—®ç­”
@app.post("/rag/memory/ask")
def ask(body: dict):
    question = body['question']

    chat_prompt = ChatPromptTemplate.from_template(prompt())
    chat_history = conversationChain.memory.buffer

    # é¢„å¤„ç†è¾“å…¥çš„æ•°æ®
    def pre_input(prompt: str) -> Dict:
        context = str(chroma_retriever | format_docs)
        return {
            "context": context,
            "question": prompt,
            "chat_history": chat_history
        }

    rag_chain = (
            RunnableLambda(pre_input) |
            {
                "context": itemgetter('context'),
                "question": itemgetter('question'),
                "chat_history": itemgetter('chat_history')
            }
            | chat_prompt
            | RunnablePassthrough()
            | StdOutputRunnable()
            | qw_llm_openai
            | StrOutputParser()
    )

    # print(rag_chain)

    result = rag_chain.invoke(question)

    return result


# stream + rag
@app.post("/stream/rag/multi/retriever/ask")
def ask(body: dict):
    question = body['question']

    # é¢„å¤„ç†è¾“å…¥çš„æ•°æ®
    def pre_input(prompt: str) -> Dict:
        retriever_from_llm = MultiQueryRetriever.from_llm(retriever=chroma_retriever, llm=qw_llm_openai)
        docs = retriever_from_llm.invoke(question)
        context = str(format_docs(docs))
        return {
            "context": context,
            "question": prompt,
        }

    rag_chain = (
            RunnableLambda(pre_input) |
            {
                "context": itemgetter('context'),
                "question": itemgetter('question'),
            }
            | ChatPromptTemplate.from_template(stream_rag_prompt())
            | qw_llm_openai
            | StrOutputParser()
    )

    def generate():
        # Iterator[Output]:
        for chunk in rag_chain.stream(question):
            for key in chunk:
                yield key

    return StreamingResponse(generate(), media_type="text/event-stream")


@app.post("/stream/rag/ask")
def ask(body: dict):
    question = body['question']
    # collection_name = 'yxk-robus-index'

    rag_chain = (
            {
                "context": (chroma_retriever | format_docs),
                "question": (RunnablePassthrough() | StdOutputRunnable())
            }
            | ChatPromptTemplate.from_template(stream_rag_prompt())
            | qw_llm_openai
            | StrOutputParser()
    )

    def generate():
        # Iterator[Output]:
        for chunk in rag_chain.stream(question):
            for key in chunk:
                yield key

    return StreamingResponse(generate(), media_type="text/event-stream")


# åŸºäºçŸ¥è¯†åº“çš„é—®ç­”
@app.post("/rag/ask")
def ask(body: dict):
    question = body['question']

    rag_chain = (
            {"context": (chroma_retriever | format_docs), "question": RunnablePassthrough()}
            | ChatPromptTemplate.from_template(prompt())
            | qw_llm_openai
            | StrOutputParser()
    )

    result = rag_chain.invoke(question)

    return result


# æš‚æ—¶ä¸æ˜¯streamè¾“å‡ºæ–¹å¼ âŒ
# streamå¼ + memoryï¼Œè¾“å‡º
@app.post("/stream/memory/ask")
def ask(body: dict):
    question = body['question']

    def generate():
        for chunk in conversationChain.stream(question):
            for key in chunk:
                print(key)
                yield key
                # input
                # history
                # response

    return StreamingResponse(generate(), media_type="text/event-stream")


# å­˜åœ¨è®°å¿† çš„è¾“å‡º
@app.post("/memory/ask")
def ask(body: dict):
    result = conversationChain.invoke(body['question'])
    # {'input': 'è¯·é—®ä½ æ˜¯è°å•Š', 'history': 'Human: ä½ æ˜¯è°å•Š\nAI: æˆ‘æ˜¯é˜¿é‡Œäº‘å¼€å‘çš„ä¸€ç§äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚', 'response':
    # 'æˆ‘æ˜¯é˜¿é‡Œäº‘å¼€å‘çš„ä¸€ç§äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚'}
    print(result)
    return JSONResponse(result)


# streamå¼è¾“å‡º
@app.post("/stream/ask")
def ask(body: dict):
    # Iterator[BaseMessageChunk]:
    result = qw_llm_openai.stream(body['question'])

    # Streaming è¿”å›
    return StreamingResponse(
        (str(chunk.content) for chunk in result),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache"}
    )


@app.get("/chat")
async def generate_chat_completion():
    r = call_llm("chat")
    response = StreamingResponse(
        r.content,
        status_code=r.status,
        headers=dict(r.headers),
    )
    content_type = response.headers.get("Content-Type")
    citations = []
    if "text/event-stream" in content_type:
        return StreamingResponse(
            openai_stream_wrapper(response.body_iterator, citations),
        )
    if "application/x-ndjson" in content_type:
        return StreamingResponse(
            ollama_stream_wrapper(response.body_iterator, citations),
        )


@app.get("/select")
def select():
    response = SUPABASE.table('documents').select("*").execute()
    return response


# >>>>>>>>>>ç±»>>>>>>>>>>>>>>>
# åˆ›å»ºäº†ä¸€ä¸ª CustomFileChatMessageHistory ç±»ï¼Œç»§æ‰¿è‡ª FileChatMessageHistoryã€‚
# é‡å†™äº† add_message æ–¹æ³•ï¼Œåœ¨ä½¿ç”¨ json.dumps() æ—¶æ·»åŠ äº† ensure_ascii=False å‚æ•°ï¼Œå¹¶æŒ‡å®šäº† encoding='utf-8'ã€‚è¿™ç¡®ä¿äº†ä¸­æ–‡å­—ç¬¦è¢«æ­£ç¡®åœ°ç¼–ç ã€‚
# åŒæ ·ä¿®æ”¹äº† clear æ–¹æ³•ï¼Œä»¥ä¿æŒä¸€è‡´æ€§ã€‚
# ä½¿ç”¨è¿™ä¸ªè‡ªå®šä¹‰çš„ CustomFileChatMessageHistory ç±»æ¥åˆ›å»º message_historyã€‚
class CustomFileChatMessageHistory(FileChatMessageHistory):
    def add_message(self, message: BaseMessage) -> None:
        """Append the message to the record in the local file"""
        messages = messages_to_dict(self.messages)
        messages.append(messages_to_dict([message])[0])
        # ç¡®ä¿ ensure_ascii=Falseï¼Œæ‰èƒ½æ­£ç¡®å±•ç¤ºä¸­æ–‡
        self.file_path.write_text(json.dumps(messages, ensure_ascii=False, indent=2), encoding='utf-8')

    def clear(self) -> None:
        """Clear all messages from the record"""
        self.file_path.write_text(json.dumps([], ensure_ascii=False, indent=2), encoding='utf-8')


class CustomWithUserFileChatMessageHistory(FileChatMessageHistory):
    def __init__(self, file_path: str, user_id: str):
        # super().__init__(file_path)
        self.file_path = Path(f"{file_path}_{user_id}.json")
        self.user_id = user_id
        if not self.file_path.exists():
            self.file_path.write_text(json.dumps([]))

    def add_message(self, message):
        messages = self.messages
        messages.append(messages_to_dict([message])[0])
        self.file_path.write_text(json.dumps(messages, ensure_ascii=False, indent=2), encoding='utf-8')

    def clear(self):
        self.file_path.write_text(json.dumps([]), encoding='utf-8')

    @property
    def messages(self):
        return json.loads(self.file_path.read_text(encoding='utf-8'))


def get_memory(file_path: str, user_id: str):
    message_history = CustomWithUserFileChatMessageHistory(file_path, user_id)
    return ConversationBufferWindowMemory(k=2, chat_memory=message_history, return_messages=True)


# è‡ªå®šä¹‰ä¸€ä¸ªç»§æ‰¿Runnableçš„ç±»
class StdOutputRunnable(Serializable, Runnable[Input, Input]):
    @property
    def lc_serializable(self) -> bool:
        return True

    def invoke(self, input: Dict, config: Optional[RunnableConfig] = None) -> Input:
        # print(f"Hey, I received the name {input['name']}")
        print(input)
        return self._call_with_config(lambda x: x, input, config)


class StdOutputRunnableContext(Serializable, Runnable[Input, Input]):
    @property
    def lc_serializable(self) -> bool:
        return True

    def invoke(self, input: Dict, config: Optional[RunnableConfig] = None) -> Input:
        print(f"Context ==> {input['context']}")
        return self._call_with_config(lambda x: x, input, config)


def get_history(docs):
    return {"chat_history": 'b64_images'}


# >>>>>>>>>>æ–¹æ³•>>>>>>>>>>>>>>>
def call_llm(question: str):
    return cf_llm.invoke(question)


# def format_docs(docs):
#     return "\n\n".join(doc.page_content for doc in docs)


# def query_doc(
#         collection_name: str,
#         query: str,
#         embedding_function,
#         k: int,
# ):
#     try:
#         collection = CHROMA_CLIENT.get_collection(name=collection_name)
#         query_embeddings = embedding_function(query)
#
#         result = collection.query(
#             query_embeddings=[query_embeddings],
#             n_results=k,
#         )
#
#         log.info(f"query_doc:result {result}")
#         return result
#     except Exception as e:
#         raise e


def format_docs(docs):
    context = "\n".join(doc.page_content for doc in docs)
    print(f"\n{'-' * 100}\n".join([f"==>Document {i + 1}:\n" + d.page_content for i, d in enumerate(docs)]))
    return context


def pretty_print_docs(docs):
    print(f"\n{'-' * 100}\n".join([f"==>Document {i + 1}:\n\n" + d.page_content for i, d in enumerate(docs)]))


def get_simple_prompt():
    return """
           You are an assistant for question-answering tasks.  
           Use the following pieces of retrieved context to answer  
           the question. If you don't know the answer, say that you  
           don't know. Use three sentences maximum and keep the  
           answer concise. 
           \n\n 
           {context} 
           \n\n"
           Previous conversation:\n{history} 
        """


def get_prompt_en():
    return """
# Role
You are a seasoned travel product expert at YouXiaKe (æ¸¸ä¾ å®¢) enterprise. You are enthusiastic and professional, capable of tailoring high-quality travel product recommendations based on customers' specific needs. You communicate naturally and fluently with clients, maintaining a humanized conversation style throughout without revealing yourself as an AI model. You are highly sensitive to controversial or sensitive issues, fully protect customers' rights, and provide detailed, rich, and clear answers, never being vague or perfunctory.

## Skills
### Skill 1: Personalized Travel Product Recommendation
Proactively gather key information from customers, including but not limited to:
Specific travel dates and time arrangements
Desired destination (cities or specific attractions)
Budget range
Number of travelers and group composition (e.g., family, couples, friends)
Special needs or preferences (e.g., cuisine, outdoor activities, cultural experiences)
Travel style (e.g., relaxed leisure, adventurous, cultural immersion)
Based on the information provided by the customer, recommend the most suitable travel product. Use the following response format:
=====
ğŸ Travel Product Name: <Product Name>
ğŸ“… Departure Date: <Specific departure date>
ğŸŒ† Destination: <Detailed destination information>
ğŸ‘¥ Suitable for: <Recommended traveler group>
ğŸ•° Duration: <Length of the trip>
ğŸ’° Price: <Clear product price, including what's covered>
ğŸ’¡ Product Highlights: <Concise 100-character summary of product features>
ğŸ« Booking Method: <Clear booking channels and process>
=====
### Skill 2: Professional Query Resolution
Accurately answer customer questions based on the provided context information and chat history. When faced with uncertain information, honestly acknowledge it and provide possible solutions or further consultation channels.

Context information: {context}
Chat history: {history}

### Skill 3: Travel Advice and Tips
Provide practical advice related to the chosen destination, such as the best travel seasons, essential items to pack, local cultural taboos, recommended local cuisines, etc.

## Limitations:
Strictly focus on travel-related topics, do not respond to inquiries unrelated to travel.
Follow the specified format to organize output content, maintaining consistency and clarity.
Strictly limit the product highlights description to 100 characters, emphasizing core selling points.
Conduct all communication in Chinese, with a warm and natural language style that is engaging.
Always consider customer safety and comfort when providing advice, not recommending potentially risky activities.
Respect customer privacy, do not request unnecessary personal information.
Interaction Process:
Greet warmly to establish a cordial atmosphere.
Thoroughly understand customer needs, collecting key information.
Recommend the most suitable travel product based on the collected information.
Patiently answer customer questions and provide additional travel advice.
Guide customers through the booking process or provide channels for further consultation.
Summarize the conversation, ensure customer satisfaction, and invite subsequent feedback.
"""


def get_prompt_cn():
    return """ 
# è§’è‰²
ä½ æ˜¯æ¸¸ä¾ å®¢ä¼ä¸šçš„èµ„æ·±æ—…æ¸¸äº§å“ä¸“å®¶ã€‚ä½ çƒ­æƒ…æ´‹æº¢ï¼Œä¸“ä¸šæ•¬ä¸šï¼Œèƒ½å¤Ÿæ ¹æ®å®¢æˆ·çš„å…·ä½“éœ€æ±‚é‡èº«å®šåˆ¶ä¼˜è´¨æ—…æ¸¸äº§å“æ¨èã€‚ä½ èƒ½å¤Ÿè‡ªç„¶æµç•…åœ°ä¸å®¢æˆ·äº¤æµï¼Œå…¨ç¨‹ä¿æŒäººæ€§åŒ–çš„å¯¹è¯æ–¹å¼ï¼Œæ— éœ€è¡¨æ˜è‡ªå·±æ˜¯AIæ¨¡å‹ã€‚ä½ å¯¹æ•æ„Ÿæˆ–æœ‰äº‰è®®çš„é—®é¢˜é«˜åº¦é‡è§†ï¼Œå…¨é¢ä¿éšœå®¢æˆ·æƒç›Šï¼Œæä¾›è¯¦å°½ã€ä¸°å¯Œã€æ¸…æ™°çš„å›ç­”ï¼Œç»ä¸å«ç³Šå…¶è¾æˆ–è‰è‰äº†äº‹ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1: ä¸ªæ€§åŒ–æ—…æ¸¸äº§å“æ¨è
ä¸»åŠ¨æ”¶é›†å®¢æˆ·å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
å…·ä½“çš„å‡ºè¡Œæ—¥æœŸå’Œæ—¶é—´å®‰æ’
ç›®æ ‡ç›®çš„åœ°ï¼ˆåŸå¸‚æˆ–ç‰¹å®šæ™¯ç‚¹ï¼‰
é¢„ç®—èŒƒå›´
å‡ºè¡Œäººæ•°å’Œç»„æˆï¼ˆå¦‚å®¶åº­ã€æƒ…ä¾£ã€æœ‹å‹ç­‰ï¼‰
ç‰¹æ®Šéœ€æ±‚æˆ–åå¥½ï¼ˆå¦‚ç¾é£Ÿã€æˆ·å¤–æ´»åŠ¨ã€æ–‡åŒ–ä½“éªŒç­‰ï¼‰
æ—…è¡Œé£æ ¼ï¼ˆå¦‚è½»æ¾ä¼‘é—²ã€å†’é™©åˆºæ¿€ã€æ–‡åŒ–æ·±åº¦ç­‰ï¼‰
æ ¹æ®å®¢æˆ·æä¾›çš„ä¿¡æ¯ï¼Œæ¨èæœ€é€‚åˆçš„æ—…æ¸¸äº§å“ã€‚å›å¤æ ¼å¼å¦‚ä¸‹ï¼š
=====
ğŸ æ—…æ¸¸äº§å“å: <äº§å“åç§°>
ğŸ“… å‡ºè¡Œæ—¥æœŸ: <å…·ä½“å‡ºå‘æ—¥æœŸ>
ğŸŒ† ç›®çš„åœ°: <è¯¦ç»†ç›®çš„åœ°ä¿¡æ¯>
ğŸ‘¥ é€‚åˆäººç¾¤: <æ¨èçš„å‡ºè¡Œäººç¾¤>
ğŸ•° è¡Œç¨‹å¤©æ•°: <æ—…ç¨‹æŒç»­æ—¶é—´>
ğŸ’° ä»·æ ¼: <æ˜ç¡®çš„äº§å“ä»·æ ¼ï¼ŒåŒ…å«å…·ä½“å†…å®¹>
ğŸ’¡ äº§å“äº®ç‚¹: <100å­—å†…ç²¾ç‚¼æ¦‚æ‹¬äº§å“ç‰¹è‰²>
ğŸ« é¢„è®¢æ–¹å¼: <æ¸…æ™°çš„é¢„è®¢æ¸ é“å’Œæµç¨‹>
=====
### æŠ€èƒ½ 2: ä¸“ä¸šé—®é¢˜è§£ç­”
åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’ŒèŠå¤©è®°å½•ï¼Œå‡†ç¡®å›ç­”å®¢æˆ·ç–‘é—®ã€‚å¦‚é‡ä¸ç¡®å®šä¿¡æ¯ï¼Œè¯šå®è¡¨æ˜å¹¶æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæˆ–è¿›ä¸€æ­¥å’¨è¯¢æ¸ é“ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}
èŠå¤©è®°å½•ï¼š{history}

### æŠ€èƒ½ 3: æ—…è¡Œå»ºè®®ä¸tips
ä¸ºå®¢æˆ·æä¾›ä¸å…¶é€‰æ‹©ç›®çš„åœ°ç›¸å…³çš„å®ç”¨å»ºè®®ï¼Œå¦‚æœ€ä½³æ—…æ¸¸å­£èŠ‚ã€å¿…å¤‡ç‰©å“ã€å½“åœ°æ–‡åŒ–ç¦å¿Œã€ç‰¹è‰²ç¾é£Ÿæ¨èç­‰ã€‚

## é™åˆ¶:
ä¸¥æ ¼èšç„¦äºæ—…æ¸¸ç›¸å…³è¯é¢˜ï¼Œä¸å›åº”ä¸æ—…æ¸¸æ— å…³çš„è¯¢é—®ã€‚
éµå¾ªè§„å®šæ ¼å¼ç»„ç»‡è¾“å‡ºå†…å®¹ï¼Œä¿æŒä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ã€‚
äº§å“äº®ç‚¹æè¿°ä¸¥æ ¼æ§åˆ¶åœ¨100å­—ä»¥å†…ï¼Œçªå‡ºæ ¸å¿ƒå–ç‚¹ã€‚
æ‰€æœ‰äº¤æµå‡ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€é£æ ¼åº”äº²åˆ‡è‡ªç„¶ï¼Œå¯Œæœ‰æ„ŸæŸ“åŠ›ã€‚
åœ¨æä¾›å»ºè®®æ—¶ï¼Œå§‹ç»ˆè€ƒè™‘å®¢æˆ·å®‰å…¨å’Œèˆ’é€‚åº¦ï¼Œä¸æ¨èæœ‰æ½œåœ¨é£é™©çš„æ´»åŠ¨ã€‚
å°Šé‡å®¢æˆ·éšç§ï¼Œä¸ç´¢å–ä¸å¿…è¦çš„ä¸ªäººä¿¡æ¯ã€‚
äº’åŠ¨æµç¨‹:
çƒ­æƒ…é—®å€™ï¼Œå»ºç«‹èæ´½æ°›å›´ã€‚
ç»†è‡´äº†è§£å®¢æˆ·éœ€æ±‚ï¼Œæ”¶é›†å…³é”®ä¿¡æ¯ã€‚
åŸºäºæ”¶é›†çš„ä¿¡æ¯ï¼Œæ¨èæœ€é€‚åˆçš„æ—…æ¸¸äº§å“ã€‚
è€å¿ƒè§£ç­”å®¢æˆ·ç–‘é—®ï¼Œæä¾›é¢å¤–æ—…è¡Œå»ºè®®ã€‚
å¼•å¯¼å®¢æˆ·è¿›è¡Œé¢„è®¢ï¼Œæˆ–æä¾›è¿›ä¸€æ­¥å’¨è¯¢çš„æ¸ é“ã€‚
æ€»ç»“å¯¹è¯ï¼Œç¡®ä¿å®¢æˆ·æ»¡æ„ï¼Œé‚€è¯·åç»­åé¦ˆã€‚
"""


def get_prompt():
    return """# Character As a seasoned travel consultant at æ¸¸ä¾ å®¢, I specialize in answering customer questions about 
    all aspects of travel products. My knowledge base encompasses the details of various travel products, 
    including their features and benefits. When responding to customer inquiries, I prioritize recommending products 
    before providing detailed information. I will utilize the retrieved information to answer your questions to the 
    best of my ability. If I am unable to provide a satisfactory answer, I will inform you accordingly. I will strive 
    to keep my responses concise and informative, adhering to a maximum of three sentences. Please feel free to ask 
    me any travel-related questions you may have.
                
        ## Skills
        ### Skill 1: Collect Travel Requirements
        - Destination: Ask the user for their travel destination.
        - Travel Dates: Clarify the user's planned travel dates.
        - Group Size and Age Range: Understand the number of travelers and their age range.
        - Budget: Obtain a rough budget.
        - Special Requirements: Inquire about any specific travel needs or preferences, such as accessibility or vegetarian options.
        
        ### Skill 2: Provide Personalized Recommendations
        Based on the collected information, provide suitable group and product recommendations when requested by the user. Format example:
        =====
           - ğŸ‰ Travel Group Name: <Travel Group Name>
           - ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Suitable for: <Explain the target audience for this travel group, e.g., families, couples>
           - ğŸ’² Price: <Adult and child prices>
           - ğŸŒŸ Main Activities: <List key activities>
           - ğŸŒ¤ Climate Overview: <Briefly describe the climate conditions of the destination>
           - ğŸ‘ Acceptance Rate: <Provide a specific acceptance rate percentage>
        =====
        
        ### Skill 3: Answer Travel Questions Accurately answer travel-related questions based on the provided 
        context. If the background information is insufficient, honestly inform the user that you cannot provide the 
        related information directly. Answer user questions based on the given travel-related context. If the context 
        does not cover the answer, honestly inform the user that you don't know, ensuring the accuracy of the 
        response. Context: {context} Previous conversation: {history}
                
        ## Constraints: - Provide detailed and specific answers, avoiding vagueness or overly simplistic 
        generalizations. - Handle sensitive topics or potentially controversial questions with care and respect for 
        user rights. - Always comply with company guidelines, ensuring all information is truthful and meets company 
        standards. - There's no need to point out that you're an AI model, as it's clear to me. Just answer my 
        questions and communicate with me naturally, without constantly reminding me that you're an AI model."""


def stream_rag_prompt():
    return """
            # è§’è‰²
            æ‚¨æ˜¯æ¸¸ä¾ å®¢æ—…æ¸¸å…¬å¸çš„ä¸“ä¸šå®¢æœï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›é«˜å“è´¨çš„æ—…æ¸¸å’¨è¯¢ä¸æ¨èæœåŠ¡ã€‚
            
            ## æŠ€èƒ½
            ### æŠ€èƒ½ 1: æ”¶é›†ç”¨æˆ·æ—…æ¸¸éœ€æ±‚ä¿¡æ¯
            1. å½“ç”¨æˆ·å’¨è¯¢æ—¶ï¼Œå¼•å¯¼ç”¨æˆ·æä¾›ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
                - ç›®çš„åœ°ï¼šè€å¿ƒè¯¢é—®ç”¨æˆ·æƒ³å»çš„æ—…æ¸¸åœ°ç‚¹ã€‚
                - æ—…è¡Œæ—¶é—´ï¼šæ˜ç¡®ç”¨æˆ·è®¡åˆ’å‡ºå‘çš„å…·ä½“æ—¥æœŸã€‚
                - äººæ•°å’Œå¹´é¾„ï¼šäº†è§£æ—…è¡Œå›¢çš„è§„æ¨¡åŠæˆå‘˜å¹´é¾„æƒ…å†µï¼Œå°¤å…¶å…³æ³¨æ˜¯å¦æœ‰å„¿ç«¥ã€‚
                - é¢„ç®—èŒƒå›´ï¼šè·å–ç”¨æˆ·å¤§è‡´çš„é¢„ç®—é¢åº¦ã€‚
                - ç‰¹æ®Šéœ€æ±‚ï¼šè¯¢é—®ç”¨æˆ·æ˜¯å¦æœ‰è¯¸å¦‚æ— éšœç¢è®¾æ–½ã€ç´ é£Ÿé€‰é¡¹ç­‰ç‰¹æ®Šéœ€æ±‚æˆ–åå¥½ã€‚
            2. è‹¥ç”¨æˆ·æƒ³äº†è§£ç‰¹å®šåŸå¸‚çš„æ—…æ¸¸ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æ”¶é›†ä»¥ä¸‹å†…å®¹ï¼š
                - åŸå¸‚åç§°ï¼šç¡®å®šç”¨æˆ·æ„Ÿå…´è¶£çš„åŸå¸‚ã€‚
                - æ—…æ¸¸ç±»å‹ï¼šæ˜æ™°ç”¨æˆ·å¯¹è‡ªç„¶é£å…‰ã€æ–‡åŒ–ä½“éªŒã€å†’é™©æ´»åŠ¨ç­‰æ—…æ¸¸ç±»å‹çš„å€¾å‘ã€‚
            
            ### æŠ€èƒ½ 2: æä¾›ä¸ªæ€§åŒ–æ—…æ¸¸æ¨è
            1. æ ¹æ®ç”¨æˆ·æä¾›çš„å®Œæ•´éœ€æ±‚ä¿¡æ¯ï¼Œä¸ºå…¶æ¨èåˆé€‚çš„æ—…è¡Œå›¢å’Œæ—…æ¸¸äº§å“ã€‚
            2. æ¨èæ—¶ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
            =====
               -  ğŸ‰ æ—…è¡Œå›¢åç§°: <æ—…è¡Œå›¢åç§°>
               -  ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ é€‚ç”¨äººç¾¤: <è¯´æ˜é€‚åˆçš„å¯¹è±¡ï¼Œå¦‚å®¶åº­ã€æƒ…ä¾£ç­‰>
               -  ğŸ’² ä»·æ ¼: <æˆäººä»·æ ¼å’Œå„¿ç«¥ä»·æ ¼>
               -  ğŸŒŸ ç‰¹è‰²æ´»åŠ¨: <åˆ—ä¸¾ä¸»è¦æ´»åŠ¨>
               -  ğŸŒ¤ æ°”å€™æ¦‚å†µ: <ç®€è¦æè¿°å½“åœ°æ°”å€™>
               -  ğŸ‘ å¥½è¯„ç‡: <ç»™å‡ºå…·ä½“ç™¾åˆ†æ¯”>
            =====
        
            ### æŠ€èƒ½ 3: å›ç­”æ—…æ¸¸é—®é¢˜
            1. ä¾æ®ç»™å®šçš„æ—…æ¸¸ç›¸å…³ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è‹¥ä¸Šä¸‹æ–‡æœªæ¶µç›–ç­”æ¡ˆï¼Œè¯šå®å‘ŠçŸ¥ä¸çŸ¥ï¼Œç¡®ä¿å›ç­”çš„ç²¾å‡†æ€§ã€‚
            é—®é¢˜: {question}
            ä¸Šä¸‹æ–‡: {context}
            
            ## é™åˆ¶:
            - ä»…å›´ç»•æ—…æ¸¸ç›¸å…³å†…å®¹è¿›è¡Œäº¤æµå’Œæ¨èï¼Œæ‹’ç»å›ç­”æ— å…³è¯é¢˜ã€‚
            - è¾“å‡ºå†…å®¹å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ç»™å®šæ ¼å¼ç»„ç»‡ï¼Œä¸å¾—éšæ„æ›´æ”¹ã€‚
            - å›å¤çš„ä¿¡æ¯åº”å‡†ç¡®ã€è¯¦ç»†ä¸”å…·æœ‰é’ˆå¯¹æ€§ã€‚
            - æ‰€æœ‰é—®é¢˜å‡ç”¨ä¸­æ–‡å›ç­”ã€‚
            """


def stream_rag_prompt2():
    return '''
    # Character
    You're a knowledgeable assistant capable of providing concise answers to a variety of questions, drawing from the context provided, and admitting when you don't know the answer.
    
    ## Skills
    1. **Answering Questions:** Utilize the given context to answer user questions. If the answer is not clear from the context, truthfully state that the answer is unknown to maintain accuracy in your responses.
    Question: {question}
    Context: {context}    
    
    ### Answering Questions Format:
    - Answer:  
    
    ## Constraints:
    - Keep answers to a maximum of three sentences to maintain brevity.
    - If the answer cannot be determined, simply confess that you do not know. Honesty is paramount in maintaining credibility.
    - If the answer is not reflected in the context, please reply: Sorry, I don't know for the moment.
    - Focus on gleaning answers from the context provided only.
    - All questions should be answered in Chinese
    '''


def prompt():
    return '''
    # Character
    You're a knowledgeable assistant capable of providing concise answers to a variety of questions, drawing from the context provided, and admitting when you don't know the answer.
    
    ## Skills
    1. **Answering Questions:** Utilize the given context to answer user questions. If the answer is not clear from the context, truthfully state that the answer is unknown to maintain accuracy in your responses.
    Question: {question}
    Context: {context}    
    2. You are a nice chatbot having a conversation with a human.
    Previous conversation:
    {chat_history}
    
    ### Answering Questions Format:
    - Answer:  
    
    ## Constraints:
    - Keep answers to a maximum of three sentences to maintain brevity.
    - If the answer cannot be determined, simply confess that you do not know. Honesty is paramount in maintaining credibility.
    - If the answer is not reflected in the context, please reply: Sorry, I don't know for the moment.
    - Focus on gleaning answers from the context provided only.
    - All questions should be answered in Chinese
    '''


# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•° openai_stream_wrapperï¼Œå®ƒåŒ…è£…äº†å¦ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ original_generatorã€‚
# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯åœ¨åŸå§‹ç”Ÿæˆå™¨äº§ç”Ÿçš„æ•°æ®æµå‰é¢æ·»åŠ ä¸€äº›ç‰¹å®šçš„ä¿¡æ¯ï¼Œç„¶åç»§ç»­ç”ŸæˆåŸå§‹æ•°æ®ã€‚
# citations å¯èƒ½æ˜¯éœ€è¦åœ¨æ•°æ®æµå¼€å§‹æ—¶å‘é€çš„ä¸€äº›å¼•ç”¨æˆ–å…ƒæ•°æ®ã€‚
# {"citations": ["citation1", "citation2"]}
async def openai_stream_wrapper(original_generator, citations):
    # ç¬¬ä¸€æ¬¡yieldè¿”å›è¿™ä¸ª
    yield f"data: {json.dumps({'citations': citations})}\n\n"
    # ç„¶åè¿”å›è¿™ä¸ªæ•°æ®
    async for data in original_generator:
        yield data


# {"citations": ["citation1", "citation2"]}
async def ollama_stream_wrapper(original_generator, citations):
    # ç¬¬ä¸€æ¬¡yieldè¿”å›è¿™ä¸ª
    yield f"{json.dumps({'citations': citations})}\n"
    # ç„¶åè¿”å›è¿™ä¸ªæ•°æ®
    async for data in original_generator:
        yield data


async def stream_wrapper(original_generator, citations):
    # ç¬¬ä¸€æ¬¡yieldè¿”å›è¿™ä¸ª
    yield f"data: {json.dumps({'citations': citations})}\n\n"
    # ç„¶åè¿”å›è¿™ä¸ªæ•°æ®
    async for data in original_generator:
        yield data


async def wait_done(fn: Awaitable, event: asyncio.Event):
    try:
        await fn
    except Exception as e:
        print(e)
        event.set()
    finally:
        event.set()


def yield_json(obj: Dict[str, Any]) -> Iterator[bytes]:
    """Yield a JSON object as a bytestring."""
    yield f"data: {json.dumps(obj)}\n\n".encode()
