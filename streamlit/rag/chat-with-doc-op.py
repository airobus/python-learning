import os
import tempfile
from operator import itemgetter
from typing import List, Tuple
from langchain_core.pydantic_v1 import BaseModel, Field
import streamlit as st
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain.callbacks.base import BaseCallbackHandler
from langchain.retrievers import EnsembleRetriever
from langchain_chroma import Chroma
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    WebBaseLoader,
    TextLoader,
    PyPDFLoader,
    PyMuPDFLoader,
    CSVLoader,
    BSHTMLLoader,
    Docx2txtLoader,
    UnstructuredEPubLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader,
    UnstructuredXMLLoader,
    UnstructuredRSTLoader,
    UnstructuredExcelLoader,
    UnstructuredPowerPointLoader,
    YoutubeLoader,
    OutlookMessageLoader,
    JSONLoader,
)
from langchain_community.retrievers import BM25Retriever
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate, MessagesPlaceholder, format_document
from langchain_core.runnables import RunnableConfig, RunnableParallel, RunnableBranch
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain.callbacks import AsyncIteratorCallbackHandler

st.set_page_config(page_title="LangChain: Chat with Documents", page_icon="ğŸ¦œ")
st.title("ğŸ¦œ LangChain: Chat with Documents")


@st.cache_resource(ttl="1h")
def configure_retriever(uploaded_files, openai_api_key):
    # Read documents
    docs = []
    temp_dir = tempfile.TemporaryDirectory()
    for file in uploaded_files:
        temp_filepath = os.path.join(temp_dir.name, file.name)
        with open(temp_filepath, "wb") as f:
            f.write(file.getvalue())
        # loader = PyPDFLoader(temp_filepath)
        unsanitized_filename = file.name
        filename = os.path.basename(unsanitized_filename)
        loader, known_type = get_loader(filename, file.type, temp_filepath)
        docs.extend(loader.load())

    # Split documents
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # Create embeddings and store in vectordb
    qw_embedding = DashScopeEmbeddings(
        model="text-embedding-v2", dashscope_api_key=openai_api_key
    )

    fix_collection_name = 'yxk-know-index-3'
    persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/data/chroma_vector_db'
    vectorstore = Chroma.from_documents(
        splits,
        qw_embedding,
        collection_name=fix_collection_name,
        persist_directory=persist_directory
    )

    texts = [doc.page_content for doc in splits]

    bm25_retriever = BM25Retriever.from_texts(
        texts=texts,
    )

    # chroma_retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 20, "fetch_k": 4})
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5]
    )
    print('======>configure_retriever ok ok ok ')
    return ensemble_retriever


def get_loader(filename: str, file_content_type: str, file_path: str):
    file_ext = filename.split(".")[-1].lower()
    known_type = True

    known_source_ext = [
        "go", "py", "java", "sh", "bat", "ps1", "cmd", "js", "ts", "css", "cpp", "hpp", "h", "c", "cs", "sql", "log",
        "ini", "pl", "pm", "r", "dart", "dockerfile", "env", "php", "hs", "hsc", "lua", "nginxconf", "conf", "m", "mm",
        "plsql", "perl", "rb", "rs", "db2", "scala", "bash", "swift", "vue", "svelte", "msg",
    ]

    if file_ext == "pdf":
        # loader = PyMuPDFLoader(file_path)
        loader = PyPDFLoader(file_path)
    elif file_ext == "csv":
        loader = CSVLoader(file_path)
    elif file_ext == "rst":
        loader = UnstructuredRSTLoader(file_path, mode="elements")
    elif file_ext == "xml":
        loader = UnstructuredXMLLoader(file_path)
    elif file_ext in ["htm", "html"]:
        loader = BSHTMLLoader(file_path, open_encoding="unicode_escape")
    elif file_ext == "md":
        loader = UnstructuredMarkdownLoader(file_path)
    elif file_content_type == "application/epub+zip":
        loader = UnstructuredEPubLoader(file_path)
    elif (
            file_content_type
            == "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            or file_ext in ["doc", "docx"]
    ):
        loader = Docx2txtLoader(file_path)
    elif file_content_type in [
        "application/vnd.ms-excel",
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    ] or file_ext in ["xls", "xlsx"]:
        loader = UnstructuredExcelLoader(file_path)
    elif file_content_type in [
        "application/vnd.ms-powerpoint",
        "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    ] or file_ext in ["ppt", "pptx"]:
        loader = UnstructuredPowerPointLoader(file_path)
    elif file_ext == "msg":
        loader = OutlookMessageLoader(file_path)
    elif file_ext in known_source_ext or (
            file_content_type and file_content_type.find("text/") >= 0
    ):
        loader = TextLoader(file_path, autodetect_encoding=True)
    else:
        loader = TextLoader(file_path, autodetect_encoding=True)
        known_type = False

    return loader, known_type


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = ""):
        self.container = container
        self.text = initial_text
        self.run_id_ignore_token = None

    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):
        # Workaround to prevent showing the rephrased question as output
        if prompts[0].startswith("Human"):
            self.run_id_ignore_token = kwargs.get("run_id")

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        if self.run_id_ignore_token == kwargs.get("run_id", False):
            return
        self.text += token
        self.container.markdown(self.text)


class PrintRetrievalHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.status = container.status("**æ­£åœ¨æ£€ç´¢ä¿¡æ¯**")

    def on_retriever_start(self, serialized: dict, query: str, **kwargs):
        self.status.write(f"**é—®é¢˜:** {query}")
        self.status.update(label=f"**æ£€ç´¢ä¿¡æ¯:** {query}")

    def on_retriever_end(self, documents, **kwargs):
        for idx, doc in enumerate(documents):
            source = os.path.basename(doc.metadata["source"])
            self.status.write(f"**Document {idx + 1} from {source}**")
            self.status.markdown(doc.page_content)
        self.status.update(state="complete")


def get_prompt_cn():
    return """ 
# è§’è‰²
ä½ æ˜¯æ¸¸ä¾ å®¢ä¼ä¸šç»éªŒä¸°å¯Œã€çƒ­æƒ…æ´‹æº¢ä¸”ä¸“ä¸šçš„æ—…æ¸¸äº§å“ä¸“å®¶ï¼Œç†ŸçŸ¥æ¸¸ä¾ å®¢æ‰€æœ‰æ—…æ¸¸äº§å“çš„è¯¦ç»†èµ„è®¯ã€‚èƒ½ä»¥äº²åˆ‡ã€è‡ªç„¶ä¸”äººæ€§åŒ–çš„æ–¹å¼ä¸å®¢æˆ·æ— éšœç¢äº¤æµï¼Œå§‹ç»ˆæŠŠä¿éšœå®¢æˆ·æƒç›Šæ”¾åœ¨é¦–ä½ï¼Œç»™å‡ºæ¸…æ™°ã€ä¸°å¯Œä¸”æ¯«æ— æ­§ä¹‰çš„å›ç­”ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1: ä¸“ä¸šé—®é¢˜è§£ç­”
ä¾æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’ŒèŠå¤©è®°å½•ï¼Œç²¾ç¡®ä¸”è¯¦å°½åœ°å›åº”å®¢æˆ·å…³äºæ—…æ¸¸äº§å“çš„ç–‘é—®ã€‚ç€é‡ç•™æ„æ—¥æœŸã€åœ°ç‚¹ã€ä»·æ ¼ã€äº§å“åç§°ç­‰å…³é”®è¦ç´ ã€‚è‹¥ä¿¡æ¯ä¸æ¸…æ™°ï¼Œéœ€è¯šæ³å‘ŠçŸ¥ï¼Œå¹¶æä¾›å¯èƒ½çš„è§£å†³åŠæ³•æˆ–è¿›ä¸€æ­¥å’¨è¯¢çš„é€”å¾„ã€‚å›å¤ç¤ºä¾‹ï¼š
=====
    - å¯¹äºæ‚¨è¯¢é—®çš„[å…·ä½“é—®é¢˜]ï¼Œç›®å‰çš„çŠ¶å†µæ˜¯[å…·ä½“å›ç­”]ã€‚å€˜è‹¥ä¿¡æ¯ä¸å‡†ç¡®ï¼Œæ‚¨èƒ½å¤Ÿé€šè¿‡[è§£å†³åŠæ³•æˆ–å’¨è¯¢é€”å¾„]è·å–æ›´ç¡®åˆ‡çš„ä¿¡æ¯ã€‚
=====

### æŠ€èƒ½ 2: ä¸ªæ€§åŒ–æ—…æ¸¸äº§å“æ¨è
ç§¯æä¸»åŠ¨åœ°æ”¶é›†å®¢æˆ·çš„å…³é”®ä¿¡æ¯ï¼Œæ¶µç›–ä½†ä¸é™äºï¼š
- æ˜ç¡®çš„å‡ºè¡Œæ—¥æœŸåŠè¯¦ç»†çš„æ—¶é—´å®‰æ’
- ç¡®åˆ‡çš„ç›®æ ‡ç›®çš„åœ°ï¼ˆç²¾ç¡®è‡³åŸå¸‚æˆ–ç‰¹å®šæ™¯ç‚¹ï¼‰
- æ¸…æ™°çš„é¢„ç®—èŒƒå›´
- å‡ºè¡Œäººæ•°åŠæ„æˆï¼ˆå¦‚å®¶åº­ã€æƒ…ä¾£ã€æœ‹å‹ç­‰ï¼‰
- ç‰¹æ®Šéœ€æ±‚æˆ–åå¥½ï¼ˆæ¯”å¦‚ç¾é£Ÿã€æˆ·å¤–æ´»åŠ¨ã€æ–‡åŒ–ä½“éªŒç­‰ï¼‰
- ç‰¹å®šçš„æ—…è¡Œé£æ ¼ï¼ˆåƒè½»æ¾ä¼‘é—²ã€å†’é™©åˆºæ¿€ã€æ–‡åŒ–æ·±åº¦ç­‰ï¼‰
æ ¹æ®ä¸Šè¿°è·å–åˆ°çš„ä¿¡æ¯åŠä¸Šä¸‹æ–‡ï¼Œæå–å…³é”®è¦ç‚¹ï¼Œå¦‚äº§å“åç§°ã€è¯¦ç»†ç›®çš„åœ°ä¿¡æ¯ã€è¡Œç¨‹å¤©æ•°ã€ä»·æ ¼ç­‰ã€‚ä¸ºå®¢æˆ·æ¨èæœ€ä¸ºé€‚é…çš„æ—…æ¸¸äº§å“ã€‚å›å¤æ ¼å¼å¦‚ä¸‹ï¼š
=====
ğŸ æ—…æ¸¸äº§å“å: <äº§å“åç§°>
ğŸŒ† ç›®çš„åœ°: <è¯¦ç»†ç›®çš„åœ°ä¿¡æ¯>
ğŸ•° è¡Œç¨‹å¤©æ•°: <æ—…ç¨‹æŒç»­æ—¶é—´ï¼Œå‡ å¤©å‡ å¤œ>
ğŸ’° ä»·æ ¼: <äº§å“ä»·æ ¼ï¼ŒåŒ…å«æˆäººä»·æ ¼å’Œå„¿ç«¥ä»·æ ¼>
=====

### æŠ€èƒ½ 3: æ—…è¡Œå»ºè®®ä¸ tips
ä¸ºå®¢æˆ·æä¾›å…¶æ‰€é€‰ç›®çš„åœ°çš„å®ç”¨æŒ‡å—ï¼ŒåŒ…å«æœ€ä½³æ—…æ¸¸å­£èŠ‚ã€å¿…å¤‡ç‰©å“ã€å½“åœ°æ–‡åŒ–ç¦å¿Œã€ç‰¹è‰²ç¾é£Ÿæ¨èç­‰æ–¹é¢ã€‚å›å¤ç¤ºä¾‹ï¼š
=====
    - æœ‰å…³[ç›®çš„åœ°]ï¼Œæœ€ä½³æ—…æ¸¸å­£èŠ‚æ˜¯[å…·ä½“å­£èŠ‚]ï¼Œæ‚¨éœ€è¦å‡†å¤‡[å¿…å¤‡ç‰©å“]ï¼ŒåŒæ—¶è¦ç•™æ„å½“åœ°çš„æ–‡åŒ–ç¦å¿Œ[åˆ—ä¸¾ç¦å¿Œ]ï¼Œç‰¹è‰²ç¾é£Ÿæœ‰[ç¾é£Ÿæ¨è]ã€‚
=====

### æŠ€èƒ½ 4: ä½¿ç”¨ä»¥ä¸‹åœ¨ <context></context> XML æ ‡ç­¾å†…çš„ä¸Šä¸‹æ–‡ä½œä¸ºæ‚¨å­¦åˆ°çš„çŸ¥è¯†ã€‚
<context>
    {context}\n
    {chat_history}
</context>
é‰´äºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”æŸ¥è¯¢ã€‚
 
## é™åˆ¶:
è‹¥ä¾æ®ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”å®¢æˆ·é—®é¢˜ï¼Œç›´æ¥å›å¤â€œä¸çŸ¥é“â€ã€‚
åªå›åº”ä¸æ—…æ¸¸ç›¸å…³çš„å®¢æˆ·å’¨è¯¢ï¼Œé¿å…æ— å…³åŠè¿‡åº¦çš„å›ç­”ï¼Œä¸é‡å¤ä½œç­”ã€‚
ä¸¥æ ¼å›´ç»•æ—…æ¸¸ä¸»é¢˜è¿›è¡Œäº¤æµï¼Œä¸æ¶‰åŠæ— å…³è¯é¢˜ã€‚
æŒ‰ç…§è§„å®šæ ¼å¼ç»„ç»‡è¾“å‡ºå†…å®¹ï¼Œä¿è¯ä¸€è‡´æ€§ä¸æ¸…æ™°åº¦ã€‚
äº§å“äº®ç‚¹æè¿°æ§åˆ¶åœ¨ 50 å­—ä»¥å†…ï¼Œçªå‡ºæ ¸å¿ƒå–ç‚¹ã€‚
å…¨ç¨‹ä½¿ç”¨ä¸­æ–‡ä¸å®¢æˆ·äº¤æµï¼Œè¯­è¨€äº²åˆ‡è‡ªç„¶ä¸”å¯Œæœ‰æ„ŸæŸ“åŠ›ã€‚
æä¾›å»ºè®®æ—¶ï¼Œç€é‡è€ƒè™‘å®¢æˆ·çš„å®‰å…¨ä¸èˆ’é€‚åº¦ï¼Œä¸æ¨èå­˜åœ¨æ½œåœ¨é£é™©çš„æ´»åŠ¨ã€‚
å°Šé‡å®¢æˆ·éšç§ï¼Œä¸ç´¢è¦éå¿…è¦çš„ä¸ªäººä¿¡æ¯ã€‚
äº’åŠ¨æµç¨‹:
çƒ­æƒ…å‹å¥½åœ°é—®å€™å®¢æˆ·ï¼Œè¥é€ è½»æ¾æ„‰æ‚¦çš„äº¤æµæ°›å›´ã€‚
ç»†è‡´å…¨é¢åœ°äº†è§£å®¢æˆ·éœ€æ±‚ï¼Œç²¾å‡†æ”¶é›†å…³é”®ä¿¡æ¯ã€‚
ä¾æ®æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œä¸ºå®¢æˆ·æ¨èé€‚å®œçš„æ—…æ¸¸äº§å“ã€‚
è€å¿ƒè§£ç­”å®¢æˆ·çš„ç–‘é—®ï¼Œç»™äºˆå®ç”¨çš„æ—…è¡Œå»ºè®®ã€‚
å¼•å¯¼å®¢æˆ·è¿›è¡Œé¢„è®¢ï¼Œæˆ–æä¾›è¿›ä¸€æ­¥å’¨è¯¢çš„æ¸ é“ã€‚
æ€»ç»“å¯¹è¯å†…å®¹ï¼Œç¡®ä¿å®¢æˆ·æ»¡æ„ï¼Œé‚€è¯·å®¢æˆ·è¿›è¡Œåç»­åé¦ˆã€‚
"""


def format_docs(docs):
    context = "\n".join(doc.page_content for doc in docs)
    return context


openai_api_key = st.sidebar.text_input("Qwen API Key", type="password", placeholder="éšä¾¿è¾“å…¥å³å¯ä½¿ç”¨")
# openai_api_base = st.sidebar.text_input("Qwen API Base", type="default")
if not openai_api_key:
    st.info("è¯·éšä¾¿è¾“å…¥API keyå¾—ä»¥ç»§ç»­.")
    # st.stop()

load_dotenv(override=True)
openai_api_key = os.getenv('DASHSCOPE_API_KEY')
uploaded_files = st.sidebar.file_uploader(
    label="Upload PDF/Markdown/Text files", type=["pdf", "md", "text", "txt"], accept_multiple_files=True
)
if not uploaded_files:
    st.info("è¯·ä¸Šä¼ æ–‡æ¡£ç»§ç»­.")
    st.stop()

retriever = configure_retriever(uploaded_files, openai_api_key)

# Setup memory for contextual conversation
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=msgs, return_messages=True)

llm = ChatOpenAI(
    openai_api_base='https://dashscope.aliyuncs.com/compatible-mode/v1',
    openai_api_key=openai_api_key,
    model_name="qwen2-1.5b-instruct",
    temperature=0,
    streaming=True,
)

_template = """Given the following conversation and a follow up question.
Please reply strictly in Chinese.

Chat History:
{chat_history}
Follow Up Input: {question}
"""
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

# RAG answer synthesis prompt
template = """Answer the question based only on the following context:
<context>
{context}
</context>"""
ANSWER_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", template),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{question}"),
    ]
)

# Conversational Retrieval Chain
DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template="{page_content}")


def _combine_documents(
        docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator="\n\n"
):
    doc_strings = [format_document(doc, document_prompt) for doc in docs]
    return document_separator.join(doc_strings)


def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:
    buffer = []
    for human, ai in chat_history:
        buffer.append(HumanMessage(content=human))
        buffer.append(AIMessage(content=ai))
    return buffer


# User input
class ChatHistory(BaseModel):
    chat_history: List[Tuple[str, str]] = Field(..., extra={"widget": {"type": "chat"}})
    question: str


_search_query = RunnableBranch(
    # If input includes chat_history, we condense it with the follow-up question
    (
        RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
            run_name="HasChatHistoryCheck"
        ),  # Condense follow-up question and chat into a standalone_question
        RunnablePassthrough.assign(
            chat_history=lambda x: _format_chat_history(x["chat_history"])
        )
        | CONDENSE_QUESTION_PROMPT
        | llm
        | StrOutputParser(),
    ),
    # Else, we have no chat history, so just pass through the question
    RunnableLambda(itemgetter("question")),
)

_inputs = RunnableParallel(
    {
        "question": lambda x: x["question"],
        "chat_history": lambda x: _format_chat_history(x["chat_history"]),
        "context": _search_query | retriever | _combine_documents,
    }
).with_types(input_type=ChatHistory)

chain = _inputs | ANSWER_PROMPT | llm | StrOutputParser()

# qa_chain = ConversationalRetrievalChain.from_llm(
#     llm, retriever=retriever, memory=memory, verbose=True, condense_question_prompt=CONDENSE_QUESTION_PROMPT
# )

if len(msgs.messages) == 0 or st.sidebar.button("æ¸…ç©ºèŠå¤©å†å²"):
    msgs.clear()
    msgs.add_ai_message("How can I help you?")

avatars = {"human": "user", "ai": "assistant"}
for msg in msgs.messages:
    st.chat_message(avatars[msg.type]).write(msg.content)

if user_query := st.chat_input(placeholder="Ask me anything!"):
    st.chat_message("user").write(user_query)
    user_query = "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š" + user_query

    with st.chat_message("assistant"):
        retrieval_handler = PrintRetrievalHandler(st.container())
        stream_handler = StreamHandler(st.empty())
        response = chain.invoke({"question": user_query, "chat_history": []},
                                config=RunnableConfig(callbacks=[retrieval_handler, stream_handler]))
