{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# ä¸­æ–‡: ç»™å®šä»¥ä¸‹å¯¹è¯å’Œåç»­é—®é¢˜ï¼Œå°†åç»­é—®é¢˜æ”¹å†™æˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œä½¿ç”¨å…¶åŸå§‹è¯­è¨€ã€‚å¦‚æœæ²¡æœ‰èŠå¤©è®°å½•ï¼Œå°±ç›´æ¥å°†é—®é¢˜æ”¹å†™æˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ã€‚\n",
    "condense_question_prompt_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. If there is no chat history, just rephrase the question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    condense_question_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ä¸­æ–‡: ä½¿ç”¨ä»¥ä¸‹æ®µè½æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "# æ¯ä¸ªæ®µè½éƒ½æœ‰ä¸€ä¸ªæ¥æºï¼Œå³æ–‡æ¡£çš„æ ‡é¢˜ã€‚å›ç­”æ—¶ï¼Œåœ¨ç­”æ¡ˆä¸‹æ–¹ä»¥ç‹¬ç‰¹çš„é¡¹ç›®ç¬¦å·åˆ—è¡¨å½¢å¼å¼•ç”¨æ‚¨å›ç­”æ‰€ä¾æ®çš„æ®µè½çš„æ¥æºåç§°ã€‚\n",
    "# å¦‚æœæ‚¨ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´æ‚¨ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚\n",
    "llm_context_prompt_template = \"\"\"\n",
    "Use the following passages to answer the user's question.\n",
    "Each passage has a SOURCE which is the title of the document. When answering, cite source name of the passages you are answering from below the answer in a unique bullet point list.\n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "----\n",
    "{context}\n",
    "----\n",
    "Question: {question}\n",
    "\"\"\"  # noqa: E501 ğŸ”¤ ä¸­æ–‡: ï¼ˆè¡¨ç¤ºå‘ŠçŸ¥ä»£ç æ£€æŸ¥å·¥å…·å¿½ç•¥ E501 è¿™ä¸ªé”™è¯¯æˆ–è­¦å‘Šï¼‰\n",
    "\n",
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(llm_context_prompt_template)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5359ed25ffccfda9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# ä¸­æ–‡: æ‚¨æ˜¯ä¸€ä¸ª AI è¯­è¨€æ¨¡å‹åŠ©æ‰‹ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ç”¨æˆ·é—®é¢˜ç”Ÿæˆäº”ä¸ªä¸åŒçš„ç‰ˆæœ¬ï¼Œä»¥ä¾¿ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚\n",
    "# é€šè¿‡å¯¹ç”¨æˆ·é—®é¢˜ç”Ÿæˆå¤šä¸ªè§†è§’ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å…‹æœåŸºäºè·ç¦»çš„ç›¸ä¼¼æ€§æœç´¢çš„ä¸€äº›é™åˆ¶ã€‚\n",
    "# ä»¥æ¢è¡Œç¬¦åˆ†éš”æä¾›è¿™äº›æ›¿ä»£é—®é¢˜ã€‚åŸå§‹é—®é¢˜: {é—®é¢˜}\"\"\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Run\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vectorstore.as_retriever(), llm, prompt=QUERY_PROMPT\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b9e8a5dc1f95e85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
