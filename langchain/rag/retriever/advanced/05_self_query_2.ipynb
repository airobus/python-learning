{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# Used to condense a question and chat history into a single question\n",
    "condense_question_prompt_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. If there is no chat history, just rephrase the question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    condense_question_prompt_template\n",
    ")\n",
    "\n",
    "# RAG Prompt to provide the context and question for LLM to answer\n",
    "# We also ask the LLM to cite the source of the passage it is answering from\n",
    "llm_context_prompt_template = \"\"\"\n",
    "Use the following passages to answer the user's question.\n",
    "Each passage has a SOURCE which is the title of the document. When answering, cite source name of the passages you are answering from below the answer in a unique bullet point list.\n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "----\n",
    "{context}\n",
    "----\n",
    "Question: {question}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(llm_context_prompt_template)\n",
    "\n",
    "# Used to build a context window from passages retrieved\n",
    "document_prompt_template = \"\"\"\n",
    "---\n",
    "NAME: {name}\n",
    "PASSAGE:\n",
    "{page_content}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "DOCUMENT_PROMPT = PromptTemplate.from_template(document_prompt_template)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe0a3af66de2d6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_elasticsearch.vectorstores import ElasticsearchStore\n",
    "\n",
    "# from .prompts import CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT, LLM_CONTEXT_PROMPT\n",
    "\n",
    "ELASTIC_CLOUD_ID = os.getenv(\"ELASTIC_CLOUD_ID\")\n",
    "ELASTIC_USERNAME = os.getenv(\"ELASTIC_USERNAME\", \"elastic\")\n",
    "ELASTIC_PASSWORD = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ELASTIC_INDEX_NAME = os.getenv(\"ELASTIC_INDEX_NAME\", \"workspace-search-example\")\n",
    "\n",
    "if ELASTIC_CLOUD_ID and ELASTIC_USERNAME and ELASTIC_PASSWORD:\n",
    "    es_connection_details = {\n",
    "        \"es_cloud_id\": ELASTIC_CLOUD_ID,\n",
    "        \"es_user\": ELASTIC_USERNAME,\n",
    "        \"es_password\": ELASTIC_PASSWORD,\n",
    "    }\n",
    "else:\n",
    "    es_connection_details = {\"es_url\": ES_URL}\n",
    "\n",
    "vecstore = ElasticsearchStore(\n",
    "    ELASTIC_INDEX_NAME,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    **es_connection_details,\n",
    ")\n",
    "# ğŸ”¤ ä¸­æ–‡: å·¥ä½œåœºæ‰€æ”¿ç­–çš„ç›®çš„å’Œè§„æ ¼ã€‚\n",
    "document_contents = \"The purpose and specifications of a workplace policy.\"\n",
    "metadata_field_info = [\n",
    "    {\"name\": \"name\", \"type\": \"string\", \"description\": \"Name of the workplace policy.\"},\n",
    "    {\n",
    "        \"name\": \"created_on\",\n",
    "        \"type\": \"date\",\n",
    "        \"description\": \"The date the policy was created in ISO 8601 date format (YYYY-MM-DD).\",  # noqa: E501\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"updated_at\",\n",
    "        \"type\": \"date\",\n",
    "        \"description\": \"The date the policy was last updated in ISO 8601 date format (YYYY-MM-DD).\",  # noqa: E501\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"location\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Where the policy text is stored. The only valid values are ['github', 'sharepoint'].\",\n",
    "        # noqa: E501\n",
    "    },\n",
    "]\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vecstore, document_contents, metadata_field_info\n",
    ")\n",
    "\n",
    "\n",
    "def _combine_documents(docs: List) -> str:\n",
    "    return \"\\n\\n\".join(format_document(doc, prompt=DOCUMENT_PROMPT) for doc in docs)\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    return \"\\n\".join(f\"Human: {human}\\nAssistant: {ai}\" for human, ai in chat_history)\n",
    "\n",
    "\n",
    "class InputType(BaseModel):\n",
    "    question: str\n",
    "    chat_history: List[Tuple[str, str]] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "standalone_question = (\n",
    "        {\n",
    "            # è·å–ç”¨æˆ·é—®é¢˜çš„å‚æ•°ï¼Œå¹¶åšç®€å•å¤„ç†\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        }\n",
    "        # å°†è¿™ä¸¤ä¸ªå­—æ®µç»„æˆçš„å­—å…¸ï¼Œä¼ é€’ç»™promptï¼Œæ ¼å¼åŒ–æˆ prompt\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        # æŠŠpromptäº¤ç»™llmä½œä¸ºè¾“å…¥\n",
    "        | llm\n",
    "        # æœ€åæŠŠllmçš„è¾“å…¥æ ¼å¼åŒ–\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def route_question(input):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return standalone_question\n",
    "    else:\n",
    "        return RunnablePassthrough()\n",
    "\n",
    "\n",
    "## å…ˆæ‰§è¡Œretrieverï¼Œè¾“å…¥æ˜¯é—®é¢˜ï¼Œè¾“å‡ºæ˜¯æ–‡æ¡£list\n",
    "## å†æ‰§è¡Œ_combine_documentsï¼Œè¾“å…¥æ˜¯æ–‡æ¡£listï¼Œè¾“å‡ºæ˜¯str\n",
    "## å†æ‰§è¡ŒLLM_CONTEXT_PROMPTï¼Œè¾“å…¥ \n",
    "# æ·»åŠ é¢å¤–å‚æ•°ï¼Œç”Ÿæˆå­—å…¸\n",
    "_context = RunnableParallel(\n",
    "    context=retriever | _combine_documents,\n",
    "    question=RunnablePassthrough(),\n",
    ")\n",
    "\n",
    "# standalone_questionï¼Œä½œç”¨æ˜¯åˆ¤æ–­ï¼Œæ˜¯ç‹¬ç«‹é—®é¢˜ï¼Œè¿˜æ˜¯å…·æœ‰è®°å¿†çš„é—®é¢˜\n",
    "chain = (\n",
    "    # å¤„ç†é—®é¢˜å¹¶ç”Ÿæˆæ–°çš„é—®é¢˜\n",
    "        standalone_question\n",
    "        # æ„å»ºpromptæ‰€éœ€çš„å­—å…¸å‚æ•°\n",
    "        | _context\n",
    "        # ç»“åˆä¸Šä¸€æ­¥çš„å­—å…¸ï¼Œç”Ÿæˆæœ€ç»ˆçš„prompt\n",
    "        | LLM_CONTEXT_PROMPT\n",
    "        # ç»“åˆä¸Šä¸€æ­¥çš„promptï¼Œç”Ÿæˆæœ€ç»ˆçš„å›ç­”\n",
    "        | llm\n",
    "        # æœ€åæ ¼å¼åŒ–è¾“å‡º\n",
    "        | StrOutputParser()\n",
    ").with_types(input_type=InputType)  # æŒ‡å®šè¾“å…¥çš„å‚æ•°ç±»å‹\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
