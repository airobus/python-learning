{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchainhub   langchain-chroma bs4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:03:47.443899Z",
     "start_time": "2024-07-04T10:03:44.203726Z"
    }
   },
   "id": "c7d6b24ebd9eaf2b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:08:29.413184Z",
     "start_time": "2024-07-04T10:08:29.409967Z"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# æ— èŠå¤©è®°å½•çš„é“¾"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214cf378f2a42e0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# åŸºæœ¬é…ç½®\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    ")\n",
    "embeddings = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=os.getenv('CF_ACCOUNT_ID'),\n",
    "    api_token=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/baai/bge-small-en-v1.5\",\n",
    ")\n",
    "\n",
    "# supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "# supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "# \n",
    "# supabase: Client = create_client(supabase_url, supabase_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:08:41.727462Z",
     "start_time": "2024-07-04T10:08:41.649313Z"
    }
   },
   "id": "acad47c9f0d9bc16",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:08:54,489:INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/langchain/history/chroma-data'\n",
    "collection_name = 'history_index'\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings,\n",
    "                                    collection_name=collection_name,\n",
    "                                    persist_directory=persist_directory)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:09:02.406143Z",
     "start_time": "2024-07-04T10:08:44.268723Z"
    }
   },
   "id": "1aba8cd4386bd9b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:09:23,037:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Task Decomposition is a method used to break down complex tasks into smaller, simpler steps. This technique involves chaining of thoughts and generating multiple thoughts per step, creating a tree structure. It can be done manually through simple prompts or automatically using task-specific instructions or human inputs.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:09:23.466481Z",
     "start_time": "2024-07-04T10:09:19.465225Z"
    }
   },
   "id": "d02a2d9e6b7c0848",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# å°†é—®é¢˜ç½®äºèƒŒæ™¯ä¸­\n",
    "\n",
    "# é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå­é“¾ï¼Œå®ƒæ¥å—å†å²æ¶ˆæ¯å’Œæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœå®ƒå¼•ç”¨äº†å†å²ä¿¡æ¯ä¸­çš„ä»»ä½•ä¿¡æ¯ï¼Œåˆ™é‡æ–°å®šä¹‰é—®é¢˜ã€‚/\n",
    "\n",
    "# æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåŒ…å«åä¸ºâ€œchat_historyâ€çš„ MessagesPlaceholder å˜é‡çš„æç¤ºç¬¦ã€‚è¿™å…è®¸æˆ‘ä»¬ä½¿ç”¨â€œchat_historyâ€è¾“å…¥é”®å‘æç¤ºç¬¦ä¼ é€’æ¶ˆæ¯åˆ—è¡¨ï¼Œ\n",
    "# è¿™äº›æ¶ˆæ¯å°†æ’å…¥åœ¨ç³»ç»Ÿæ¶ˆæ¯ä¹‹åï¼ŒåŒ…å«æœ€æ–°é—®é¢˜çš„äººå·¥æ¶ˆæ¯ä¹‹å‰ã€‚\n",
    "\n",
    "# è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æ­¤æ­¥éª¤ä¸­ä½¿ç”¨äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•°create_history_aware_retrieverï¼Œå®ƒç®¡ç† chat_history ä¸ºç©ºçš„æƒ…å†µï¼Œ\n",
    "# å¦åˆ™æŒ‰é¡ºåºåº”ç”¨ prompt | llm | StrOutputParser() | retriever ã€‚\n",
    "\n",
    "# create_history_aware_retriever æ„é€ äº†ä¸€ä¸ªé“¾ï¼Œæ¥å—é”® input å’Œ chat_history ä½œä¸ºè¾“å…¥ï¼Œå¹¶å…·æœ‰ä¸æ£€ç´¢å™¨ç›¸åŒçš„è¾“å‡ºæ¨¡å¼ã€‚"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "553acbf1327ec533"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "# from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# =====\n",
    "# ä¸­æ–‡: ç»™å®šä¸€ä¸ªèŠå¤©å†å²è®°å½•å’Œæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼Œè¯¥é—®é¢˜å¯èƒ½å‚è€ƒäº†èŠå¤©å†å²è®°å½•ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œåˆ¶å®šä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œ\n",
    "# è¿™ä¸ªé—®é¢˜æ— éœ€èŠå¤©å†å²è®°å½•å°±èƒ½ç†è§£ã€‚ä¸è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¦‚æœéœ€è¦ï¼Œåªéœ€é‡æ–°è¡¨è¿°å®ƒï¼Œå¦åˆ™å°±æŒ‰åŸæ ·è¿”å›ã€‚\n",
    "# =====\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    qw_llm_openai, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:16:28.854807Z",
     "start_time": "2024-07-04T10:16:28.845826Z"
    }
   },
   "id": "d2740084d0870258",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# å¸¦èŠå¤©è®°å½•çš„é“¾\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬å¯ä»¥å»ºç«‹å®Œæ•´çš„QAé“¾ã€‚\n",
    "\n",
    "# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨create_stuff_documents_chainç”Ÿæˆä¸€ä¸ª question_answer_chain ï¼Œ\n",
    "# è¾“å…¥é”®ä¸º context ã€ chat_history å’Œ input --å®ƒæ¥å—æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä»¥åŠä¼šè¯å†å²å’ŒæŸ¥è¯¢ï¼Œä»¥ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆã€‚\n",
    "\n",
    "# æˆ‘ä»¬ä½¿ç”¨create_retrieval_chainæ„å»ºæœ€ç»ˆçš„ rag_chain ã€‚è¿™ä¸ªé“¾æŒ‰é¡ºåºåº”ç”¨ history_aware_retriever å’Œ question_answer_chain ï¼Œ\n",
    "# ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œä¿ç•™äº†ä¸­é—´è¾“å‡ºï¼Œ\n",
    "# å¦‚æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚å®ƒæœ‰è¾“å…¥é”® input å’Œ chat_history ï¼Œå¹¶åœ¨å…¶è¾“å‡ºä¸­åŒ…æ‹¬ input ã€ chat_history ã€ context å’Œ answer ã€‚"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca6d0a5166ea6fb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# =====\n",
    "# -  ğŸ”¤ ä¸­æ–‡: ä½ æ˜¯å›ç­”é—®é¢˜ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯å¹¶ä¿æŒç­”æ¡ˆç®€æ´ã€‚\n",
    "# =====\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(qw_llm_openai, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:22:12.463430Z",
     "start_time": "2024-07-04T10:22:12.389805Z"
    }
   },
   "id": "d5a43820fb67f501",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:22:51,014:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:22:51.610113Z",
     "start_time": "2024-07-04T10:22:47.696996Z"
    }
   },
   "id": "8968e19cc02f27a1",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'What is Task Decomposition?',\n 'chat_history': [HumanMessage(content='What is Task Decomposition?'),\n  'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.'],\n 'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: â€¦ step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:23:00.045921Z",
     "start_time": "2024-07-04T10:23:00.040431Z"
    }
   },
   "id": "a98a2089ae8020e5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:29:50,023:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:29:52,439:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to do task decomposition:\n",
      "\n",
      "  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\n",
      "  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\n",
      "  3. Human inputs to guide the decomposition process.\n",
      "\n",
      "Overall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.\n"
     ]
    }
   ],
   "source": [
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:29:53.424326Z",
     "start_time": "2024-07-04T10:29:49.670006Z"
    }
   },
   "id": "fb8f4e645b7dcf3d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='What is Task Decomposition?'),\n 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:00.465513Z",
     "start_time": "2024-07-04T10:30:00.454698Z"
    }
   },
   "id": "f2d0547ee60f7715",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=second_question), ai_msg_2[\"answer\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:32.628627Z",
     "start_time": "2024-07-04T10:30:32.621773Z"
    }
   },
   "id": "11aa3f92ddf14208",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='What is Task Decomposition?'),\n 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.',\n HumanMessage(content='What are common ways of doing it?'),\n 'There are several ways to do task decomposition:\\n\\n  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\\n  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\\n  3. Human inputs to guide the decomposition process.\\n\\nOverall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:36.464942Z",
     "start_time": "2024-07-04T10:30:36.459384Z"
    }
   },
   "id": "297a0d24544e5ada",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:34:28,612:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:34:32,043:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGæ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡ä½¿ç”¨è§„åˆ™æ¥ç”Ÿæˆæ–‡æœ¬ã€‚å®ƒçš„åå­—æ˜¯Random Access Grammarï¼Œå…¶ä¸­Accessä»£è¡¨äº†å¯ä»¥è®¿é—®çš„èµ„æºï¼ˆå¦‚å•è¯è¡¨ã€è¯å…¸ç­‰ï¼‰ï¼ŒGrammaråˆ™è¡¨ç¤ºç”Ÿæˆè§„åˆ™ã€‚\n",
      "\n",
      "RAGæ¨¡å‹é€šå¸¸ç”¨äºè‡ªåŠ¨é—®ç­”ç³»ç»Ÿä¸­ï¼Œå› ä¸ºå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆç­”æ¡ˆã€‚å®ƒå¯ä»¥å°†é—®é¢˜åˆ†è§£æˆä¸€ç³»åˆ—æ­¥éª¤ï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è¾“å…¥æˆ–è¾“å‡ºã€‚è¿™ä½¿å¾—æœºå™¨èƒ½å¤Ÿç†è§£å’Œå›ç­”å¤æ‚çš„é—®é¢˜ï¼Œè€Œä¸éœ€è¦äººç±»çš„å¸®åŠ©ã€‚\n"
     ]
    }
   ],
   "source": [
    "second_question = \"ä»€ä¹ˆæ˜¯rag?\"\n",
    "ai_msg_3 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_3[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:34:33.158512Z",
     "start_time": "2024-07-04T10:34:28.247082Z"
    }
   },
   "id": "91b5470e2ae8a047",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'ä»€ä¹ˆæ˜¯rag?',\n 'chat_history': [HumanMessage(content='What is Task Decomposition?'),\n  'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.',\n  HumanMessage(content='What are common ways of doing it?'),\n  'There are several ways to do task decomposition:\\n\\n  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\\n  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\\n  3. Human inputs to guide the decomposition process.\\n\\nOverall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.'],\n 'context': [Document(page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'RAGæ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡ä½¿ç”¨è§„åˆ™æ¥ç”Ÿæˆæ–‡æœ¬ã€‚å®ƒçš„åå­—æ˜¯Random Access Grammarï¼Œå…¶ä¸­Accessä»£è¡¨äº†å¯ä»¥è®¿é—®çš„èµ„æºï¼ˆå¦‚å•è¯è¡¨ã€è¯å…¸ç­‰ï¼‰ï¼ŒGrammaråˆ™è¡¨ç¤ºç”Ÿæˆè§„åˆ™ã€‚\\n\\nRAGæ¨¡å‹é€šå¸¸ç”¨äºè‡ªåŠ¨é—®ç­”ç³»ç»Ÿä¸­ï¼Œå› ä¸ºå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆç­”æ¡ˆã€‚å®ƒå¯ä»¥å°†é—®é¢˜åˆ†è§£æˆä¸€ç³»åˆ—æ­¥éª¤ï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è¾“å…¥æˆ–è¾“å‡ºã€‚è¿™ä½¿å¾—æœºå™¨èƒ½å¤Ÿç†è§£å’Œå›ç­”å¤æ‚çš„é—®é¢˜ï¼Œè€Œä¸éœ€è¦äººç±»çš„å¸®åŠ©ã€‚'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:34:45.875374Z",
     "start_time": "2024-07-04T10:34:45.866203Z"
    }
   },
   "id": "9b8befd2346ba913",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:36:02,432:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Randomly Generated Assistant) æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒç»“åˆäº†ç”Ÿæˆå¼æ¨¡å‹å’Œæœ‰ç›‘ç£å­¦ä¹ ã€‚RAG ä½¿ç”¨éšæœºæ–‡æœ¬ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨å·²æœ‰çš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒã€‚\n",
      "\n",
      "RAG çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ä¸è§£é‡Šå™¨ï¼ˆInverterï¼‰ç»„åˆåœ¨ä¸€èµ·ï¼Œä½¿å¾—åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¯ä»¥åŒæ—¶è€ƒè™‘ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œè¯­è¨€ç»“æ„çš„æ­£ç¡®æ€§ã€‚è¿™ç§è®¾è®¡æ–¹å¼å…è®¸ RAG åœ¨å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡æ—¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å†…å®¹ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æŒ‡å®šæ¯ä¸€æ­¥çš„é€»è¾‘æˆ–è§„åˆ™ã€‚\n",
      "\n",
      "RAG æ¨¡å‹é€šå¸¸ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç”Ÿæˆå™¨å’Œè§£é‡Šå™¨ã€‚ç”Ÿæˆå™¨è´Ÿè´£æ ¹æ®ç»™å®šçš„åˆå§‹æ–‡æœ¬ç”Ÿæˆåç»­çš„æ–‡æœ¬ï¼›è§£é‡Šå™¨åˆ™ç”¨äºè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å¹¶æä¾›åé¦ˆã€‚è¿™ç§è®¾è®¡æ¨¡å¼ä½¿å¾— RAG èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ç°æœ‰çš„è¯­è¨€èµ„æºï¼Œå¦‚å¤§é‡çš„è®­ç»ƒæ•°æ®ã€é¢„è®­ç»ƒæ¨¡å‹ç­‰ï¼Œä»è€Œå¿«é€Ÿæé«˜å…¶æ€§èƒ½ã€‚\n"
     ]
    }
   ],
   "source": [
    "second_question = \"ä»€ä¹ˆæ˜¯rag?\"\n",
    "ai_msg_4 = rag_chain.invoke({\"input\": second_question, \"chat_history\": []})\n",
    "\n",
    "print(ai_msg_4[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:36:04.479257Z",
     "start_time": "2024-07-04T10:35:59.796227Z"
    }
   },
   "id": "d252cb528604f79d",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'ä»€ä¹ˆæ˜¯rag?',\n 'chat_history': [],\n 'context': [Document(page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'RAG (Randomly Generated Assistant) æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒç»“åˆäº†ç”Ÿæˆå¼æ¨¡å‹å’Œæœ‰ç›‘ç£å­¦ä¹ ã€‚RAG ä½¿ç”¨éšæœºæ–‡æœ¬ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨å·²æœ‰çš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒã€‚\\n\\nRAG çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ä¸è§£é‡Šå™¨ï¼ˆInverterï¼‰ç»„åˆåœ¨ä¸€èµ·ï¼Œä½¿å¾—åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¯ä»¥åŒæ—¶è€ƒè™‘ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œè¯­è¨€ç»“æ„çš„æ­£ç¡®æ€§ã€‚è¿™ç§è®¾è®¡æ–¹å¼å…è®¸ RAG åœ¨å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡æ—¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å†…å®¹ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æŒ‡å®šæ¯ä¸€æ­¥çš„é€»è¾‘æˆ–è§„åˆ™ã€‚\\n\\nRAG æ¨¡å‹é€šå¸¸ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç”Ÿæˆå™¨å’Œè§£é‡Šå™¨ã€‚ç”Ÿæˆå™¨è´Ÿè´£æ ¹æ®ç»™å®šçš„åˆå§‹æ–‡æœ¬ç”Ÿæˆåç»­çš„æ–‡æœ¬ï¼›è§£é‡Šå™¨åˆ™ç”¨äºè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å¹¶æä¾›åé¦ˆã€‚è¿™ç§è®¾è®¡æ¨¡å¼ä½¿å¾— RAG èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ç°æœ‰çš„è¯­è¨€èµ„æºï¼Œå¦‚å¤§é‡çš„è®­ç»ƒæ•°æ®ã€é¢„è®­ç»ƒæ¨¡å‹ç­‰ï¼Œä»è€Œå¿«é€Ÿæé«˜å…¶æ€§èƒ½ã€‚'}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:36:11.682227Z",
     "start_time": "2024-07-04T10:36:11.671170Z"
    }
   },
   "id": "53756fc3fed7716c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # å›è¿”æ¥æº\n",
    "\n",
    "# é€šå¸¸åœ¨é—®ç­”åº”ç”¨ç¨‹åºä¸­ï¼Œå‘ç”¨æˆ·æ˜¾ç¤ºç”¨äºç”Ÿæˆç­”æ¡ˆçš„æ¥æºæ˜¯å¾ˆé‡è¦çš„ã€‚\n",
    "# LangChainå†…ç½®çš„ create_retrieval_chain ä¼šå°†æ£€ç´¢åˆ°çš„æºæ–‡æ¡£ä¼ æ’­åˆ° \"context\" é”®ä¸­çš„è¾“å‡ºï¼š"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d92ecc51463dfb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "for document in ai_msg_4[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:37:49.244206Z",
     "start_time": "2024-07-04T10:37:49.238333Z"
    }
   },
   "id": "671706217f6e6a94",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†å¦‚ä½•æ·»åŠ åº”ç”¨ç¨‹åºé€»è¾‘æ¥åˆå¹¶å†å²è¾“å‡ºï¼Œä½†æˆ‘ä»¬ä»ç„¶æ‰‹åŠ¨æ›´æ–°èŠå¤©å†å²å¹¶å°†å…¶æ’å…¥åˆ°æ¯ä¸ªè¾“å…¥ä¸­ã€‚\n",
    "# åœ¨ä¸€ä¸ªçœŸå®çš„Q&Aåº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›æ–¹æ³•æ¥ä¿å­˜èŠå¤©è®°å½•ï¼Œä»¥åŠä¸€äº›è‡ªåŠ¨æ’å…¥å’Œæ›´æ–°èŠå¤©è®°å½•çš„æ–¹æ³•ã€‚\n",
    "\n",
    "# ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨é€”ï¼š\n",
    "\n",
    "# BaseChatMessageHistoryï¼šå­˜å‚¨èŠå¤©è®°å½•ã€‚\n",
    "# RunnableWithMessageHistoryï¼šLCELé“¾å’Œ BaseChatMessageHistory çš„åŒ…è£…å™¨ï¼Œç”¨äºå¤„ç†å°†èŠå¤©å†å²æ³¨å…¥è¾“å…¥å¹¶åœ¨æ¯æ¬¡è°ƒç”¨åæ›´æ–°å®ƒã€‚\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a994a87253760509"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# å®ç°ç¬¬äºŒä¸ªé€‰é¡¹çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œå…¶ä¸­èŠå¤©å†å²è®°å½•å­˜å‚¨åœ¨ä¸€ä¸ªç®€å•çš„dictä¸­ã€‚"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c8ebc2562856b38"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import bs4\n",
    "# from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/langchain/history/chroma-data'\n",
    "collection_name = 'history2_index'\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings,\n",
    "                                    collection_name=collection_name,\n",
    "                                    persist_directory=persist_directory)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    qw_llm_openai, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(qw_llm_openai, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "# æœ‰çŠ¶æ€åœ°ç®¡ç†èŠå¤©è®°å½•\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# RunnableWithMessageHistory å…è®¸æˆ‘ä»¬å°†æ¶ˆæ¯å†å²æ·»åŠ åˆ°æŸäº›ç±»å‹çš„é“¾ä¸­ã€‚å®ƒåŒ…è£…å¦ä¸€ä¸ªRunnableå¹¶ç®¡ç†å®ƒçš„èŠå¤©æ¶ˆæ¯å†å²è®°å½•ã€‚\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:47:56.617924Z",
     "start_time": "2024-07-04T10:47:48.572882Z"
    }
   },
   "id": "9eb7dfd85cf02b45",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:48:18,576:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:19.002492Z",
     "start_time": "2024-07-04T10:48:15.597848Z"
    }
   },
   "id": "85a7f18a1dda6f2b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.')])}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:39.746509Z",
     "start_time": "2024-07-04T10:48:39.735906Z"
    }
   },
   "id": "5478958840ac4d71",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:48:48,893:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:48:52,750:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'There are several common ways to perform task decomposition:\\n\\n  * Using LLMs with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ.\"\\n  * Using task-specific instructions such as \"Write a story outline\" for writing a novel.\\n  * Using human inputs to guide the decomposition process.\\n\\nThese approaches help to make the task more manageable and allow the agent to focus on specific aspects of the task while still completing all necessary steps.'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:53.390614Z",
     "start_time": "2024-07-04T10:48:48.438665Z"
    }
   },
   "id": "e171c0eebda44722",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.'), HumanMessage(content='What are common ways of doing it?'), AIMessage(content='There are several common ways to perform task decomposition:\\n\\n  * Using LLMs with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ.\"\\n  * Using task-specific instructions such as \"Write a story outline\" for writing a novel.\\n  * Using human inputs to guide the decomposition process.\\n\\nThese approaches help to make the task more manageable and allow the agent to focus on specific aspects of the task while still completing all necessary steps.')])}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:58.380478Z",
     "start_time": "2024-07-04T10:48:58.372114Z"
    }
   },
   "id": "d69b4423e4dd09fe",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# æ–‡æ¡£åœ°å€\n",
    "# https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7184fc2206aa04a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
