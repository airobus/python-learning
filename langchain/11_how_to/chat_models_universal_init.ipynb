{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "ğŸª„ åœ¨ä¸€è¡Œä»£ç ä¸­åˆå§‹åŒ–ä»»ä½•æ¨¡å‹\n",
    "LangChain æœ‰å¾ˆå¤šèŠå¤©æ¨¡å‹é›†æˆï¼Œå¯èƒ½å¾ˆéš¾è®°ä½å¦‚ä½•å¯¼å…¥æ¯ä¸ªé›†æˆã€‚"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9705406023c185b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Basic usage\n",
    "# Returns a langchain_openai.ChatOpenAI instance.\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)\n",
    "# Returns a langchain_anthropic.ChatAnthropic instance.\n",
    "claude_opus = init_chat_model(\n",
    "    \"claude-3-opus-20240229\", model_provider=\"anthropic\", temperature=0\n",
    ")\n",
    "# Returns a langchain_google_vertexai.ChatVertexAI instance.\n",
    "gemini_15 = init_chat_model(\n",
    "    \"gemini-1.5-pro\", model_provider=\"google_vertexai\", temperature=0\n",
    ")\n",
    "\n",
    "# Since all model integrations implement the ChatModel interface, you can use them in the same way.\n",
    "print(\"GPT-4o: \" + gpt_4o.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Claude Opus: \" + claude_opus.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Gemini 1.5: \" + gemini_15.invoke(\"what's your name\").content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# æ¨ç†æ¨¡å‹æä¾›ç¨‹åº\n",
    "\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", temperature=0)\n",
    "claude_opus = init_chat_model(\"claude-3-opus-20240229\", temperature=0)\n",
    "gemini_15 = init_chat_model(\"gemini-1.5-pro\", temperature=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1846cc02aeaa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¯é…ç½®æ¨¡å‹\n",
    "# æ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®š configurable_fields æ¥åˆ›å»ºè¿è¡Œæ—¶å¯é…ç½®çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨æœªæŒ‡å®š model å€¼ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹ â€œmodelâ€ å’Œ â€œmodel_providerâ€ æ˜¯å¯é…ç½®çš„ã€‚\n",
    "\n",
    "configurable_model = init_chat_model(temperature=0)\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\", config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
    ")\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\", config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2012beea4f0d3617"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# å…·æœ‰é»˜è®¤å€¼çš„å¯é…ç½®æ¨¡å‹\n",
    "# æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå…·æœ‰é»˜è®¤æ¨¡å‹å€¼çš„å¯é…ç½®æ¨¡å‹ï¼ŒæŒ‡å®šå“ªäº›å‚æ•°æ˜¯å¯é…ç½®çš„ï¼Œå¹¶ä¸ºå¯é…ç½®çš„å‚æ•°æ·»åŠ å‰ç¼€ï¼š\n",
    "\n",
    "first_llm = init_chat_model(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
    "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
    ")\n",
    "\n",
    "first_llm.invoke(\"what's your name\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2c11182c865632"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "first_llm.invoke(\n",
    "    \"what's your name\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"first_model\": \"claude-3-5-sonnet-20240620\",\n",
    "            \"first_temperature\": 0.5,\n",
    "            \"first_max_tokens\": 100,\n",
    "        }\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114ef50b9d179fc5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ä»¥å£°æ˜æ–¹å¼ä½¿ç”¨å¯é…ç½®æ¨¡å‹\n",
    "# æˆ‘ä»¬å¯ä»¥åœ¨å¯é…ç½®æ¨¡å‹ä¸Šè°ƒç”¨ bind_toolsã€with_structured_outputã€with_configurable ç­‰å£°æ˜æ€§æ“ä½œï¼Œ\n",
    "# å¹¶ä»¥ä¸å¸¸è§„å®ä¾‹åŒ–èŠå¤©æ¨¡å‹å¯¹è±¡ç›¸åŒçš„æ–¹å¼é“¾æ¥å¯é…ç½®æ¨¡å‹ã€‚\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "    \"\"\"Get the current population in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm = init_chat_model(temperature=0)\n",
    "llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
    ").tool_calls"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e827e5b079cd3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\",\n",
    "    config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}},\n",
    ").tool_calls"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d5d43a06e2e5ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
